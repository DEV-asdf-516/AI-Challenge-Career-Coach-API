#!/usr/bin/env python3
import os, sys, json, glob, time, shutil, subprocess, threading, hashlib
import urllib.request, urllib.error
import requests  # <-- add requests for streaming upload

HOST = os.environ.get("OLLAMA_HOST_URL", "http://ollama:11434").rstrip("/")
NAME = os.environ.get("OLLAMA_MODEL", "benedict/linkbricks-llama3.1-korean:8b")

def log(msg): print(msg, flush=True)

# ---------- HTTP helpers ----------
def _http_post(url, payload=None, headers=None, stream=False):
    data = None if payload is None else json.dumps(payload).encode("utf-8")
    req = urllib.request.Request(url, data=data, headers=headers or {"Content-Type":"application/json"})
    if stream:
        return urllib.request.urlopen(req)  # caller consumes lines
    else:
        with urllib.request.urlopen(req) as r:
            return r.read()

def stream_post(path, payload):
    try:
        with _http_post(f"{HOST}{path}", payload, stream=True) as r:
            for raw in r:
                line = raw.decode("utf-8", "ignore").strip()
                if not line: continue
                try:
                    obj = json.loads(line)
                except Exception:
                    log(f"[ollama] {line}")
                    continue
                if "status" in obj:
                    st = obj.get("status")
                    completed, total = obj.get("completed"), obj.get("total")
                    pct = f" {int(completed/total*100)}%" if (completed and total) else ""
                    log(f"ðŸŸ¡ {st}{pct}")
                if obj.get("error"): raise RuntimeError(obj["error"])
                if obj.get("success") is True:
                    log("âœ… ì™„ë£Œ"); return obj
        log("â„¹ï¸ ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ"); return None
    except urllib.error.HTTPError as e:
        body = e.read().decode("utf-8", "ignore")
        raise RuntimeError(f"Ollama HTTP {e.code} @ {path}: {body}")
    except urllib.error.URLError as e:
        raise RuntimeError(f"Ollama ì—°ê²° ì‹¤íŒ¨ @ {path}: {e}")

def post_ok(path, payload):
    _http_post(f"{HOST}{path}", payload)
    return True

def exists(model_name):
    try:
        post_ok("/api/show", {"model": model_name})
        return True
    except Exception:
        return False

# ---------- utils ----------
def human(n):
    for u in ["B","KB","MB","GB","TB"]:
        if n < 1024: return f"{n:.1f}{u}"
        n /= 1024
    return f"{n:.1f}PB"

def sha256_of_file(fp, chunk=8*1024*1024):
    h = hashlib.sha256()
    with open(fp, "rb") as f:
        while True:
            b = f.read(chunk)
            if not b: break
            h.update(b)
    return "sha256:" + h.hexdigest()

def blob_exists(digest):
    # HEAD /api/blobs/:digest
    url = f"{HOST}/api/blobs/{digest}"
    try:
        r = requests.head(url, timeout=30)
        return r.status_code == 200
    except Exception:
        return False

def upload_blob_with_digest(fp, digest):
    # POST /api/blobs/:digest (raw bytes)
    url = f"{HOST}/api/blobs/{digest}"
    with open(fp, "rb") as f:
        r = requests.post(url, data=f, headers={"Content-Type":"application/octet-stream"}, timeout=None)
    r.raise_for_status()

# ---------- main ----------
def main():
    log(f"ðŸ”§ OLLAMA_HOST_URL = {HOST}")
    log(f"ðŸ”§ OLLAMA_MODEL    = {NAME}")

    os.makedirs("/models/.tmp", exist_ok=True)
    os.environ.setdefault("HF_HUB_TEMP_DIR", "/models/.tmp")
    os.environ.setdefault("HF_HUB_ENABLE_PROGRESS_BARS", "1")
    os.environ.setdefault("HF_HUB_ENABLE_HF_TRANSFER", "1")  # if installed
    os.environ.setdefault("HF_HUB_DISABLE_TELEMETRY", "1")

    try:
        total, used, free = shutil.disk_usage("/models")
        log(f"ðŸ’¾ /models free: {human(free)} / total: {human(total)}")
    except Exception as e:
        log(f"âš ï¸ disk usage í™•ì¸ ì‹¤íŒ¨: {e}")

    if NAME == "llama3-instruct-kor-8b-q4km":
        if exists(NAME):
            log(f"âœ… ì´ë¯¸ ìƒì„±ëœ ëª¨ë¸: {NAME}")
            return

        # ensure huggingface_hub
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "show", "huggingface_hub"])
        except subprocess.CalledProcessError:
            subprocess.check_call([sys.executable, "-m", "pip", "install", "--no-cache-dir", "huggingface_hub==0.24.6"])

        from huggingface_hub import snapshot_download, login, HfApi
        token = os.environ.get("HF_TOKEN") or None
        if token:
            try:
                login(token=token); log("ðŸ”‘ HF í† í° ë¡œê·¸ì¸ ì„±ê³µ")
            except Exception as e:
                log(f"âš ï¸ HF í† í° ë¡œê·¸ì¸ ê²½ê³ : {e}")

        target_dir = "/models/llama3-instruct-kor-8b-q4km"
        repo_id    = "QuantFactory/Llama-3.1-Korean-8B-Instruct-GGUF"
        pattern    = "*Q4_K_M.gguf"
        os.makedirs(target_dir, exist_ok=True)

        log(f"ðŸ“¥ HF ë‹¤ìš´ë¡œë“œ ì‹œìž‘: {repo_id} â†’ {target_dir} (pattern='{pattern}')")
        t0 = time.time()
        snapshot_download(
            repo_id=repo_id,
            local_dir=target_dir,
            allow_patterns=[pattern],
            local_dir_use_symlinks=False,
        )
        dt = time.time() - t0
        log(f"âœ… HF ë‹¤ìš´ë¡œë“œ ì™„ë£Œ ({dt:.1f}s)")

        ggufs = glob.glob(os.path.join(target_dir, pattern))
        if not ggufs: raise RuntimeError("GGUF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        gguf = ggufs[0]
        fname = os.path.basename(gguf)
        sz = os.path.getsize(gguf)
        log(f"ðŸ“„ GGUF íŒŒì¼: {gguf} ({human(sz)})")

        # --- blobs + create(files + modelfile) ---
        log("ðŸ” SHA256 ê³„ì‚° ì¤‘...")
        digest = sha256_of_file(gguf)
        log(f"âœ… SHA256: {digest}")

        if blob_exists(digest):
            log("â„¹ï¸ blob ì´ë¯¸ ì¡´ìž¬ â†’ ì—…ë¡œë“œ ìŠ¤í‚µ")
        else:
            log("â¬†ï¸ blob ì—…ë¡œë“œ ì‹œìž‘ (streaming)")
            upload_blob_with_digest(gguf, digest)
            log("âœ… blob ì—…ë¡œë“œ ì™„ë£Œ")

        # /api/create í˜¸ì¶œ
        payload = {
            "model": NAME,
            "files": { fname: digest }  # â† í•„ìˆ˜
        }

        log("ðŸ—ï¸ Ollama ëª¨ë¸ ìƒì„± ì‹œìž‘ (/api/create)")
        stream_post("/api/create", payload)
        log(f"âœ… ëª¨ë¸ ìƒì„± ì™„ë£Œ: {NAME}")

    else:
        if exists(NAME):
            log(f"âœ… ì´ë¯¸ ì¡´ìž¬í•˜ëŠ” ëª¨ë¸(ì›ê²©): {NAME}")
            return
        log(f"ðŸŒ Ollama pull ì‹œìž‘: {NAME}")
        stream_post("/api/pull", {"model": NAME})
        log(f"âœ… pull ì™„ë£Œ: {NAME}")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        log(f"âŒ ì˜¤ë¥˜: {e}")
        sys.exit(1)
