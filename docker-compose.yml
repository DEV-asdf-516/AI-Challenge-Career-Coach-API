services:
  # Ollama 서버
  ollama:
    image: ollama/ollama:latest
    container_name: ollama-server
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
      - models_data:/models
    environment:
      - OLLAMA_HOST=0.0.0.0
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 60s
      retries: 3
      start_period: 30s

  # Spring Boot API 서버
  resume-coach-api:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        - PORT=${PORT:-9070}
    container_name: coach
    ports:
      - "${PORT:-9070}:${PORT:-9070}"
    depends_on:
      ollama:
        condition: service_healthy
      model-setup:
        condition: service_completed_successfully
    environment:
      # 데이터베이스 환경변수
      - DB_NAME=${DB_NAME}
      - DB_USERNAME=${DB_USERNAME}
      - DB_PASSWORD=${DB_PASSWORD}

      # 서버 환경변수
      - PORT=${PORT:-9070}

      # Ollama 환경변수
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://ollama:11434}
      - OLLAMA_MODEL=${OLLAMA_MODEL:-benedict/linkbricks-llama3.1-korean:8b}

      # Swagger 환경변수
      - SWAGGER_SERVER_URL=${SWAGGER_SERVER_URL:-http://localhost}

    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${PORT:-9070}/actuator/health"]
      interval: 10s
      timeout: 60s
      retries: 3
      start_period: 60s

  # 모델 초기화 서비스
  model-setup:
    build: # ⬅️ 로컬에서 모델-세팅 이미지 생성
      context: .
      dockerfile: Dockerfile.model-setup
    container_name: ollama-model-setup
    depends_on:
      ollama:
        condition: service_healthy
    environment:
      - OLLAMA_MODEL=${OLLAMA_MODEL:-benedict/linkbricks-llama3.1-korean:8b}
      - OLLAMA_HOST_URL=http://ollama:11434
      - HF_TOKEN=${HF_TOKEN:-}
    volumes:
      - models_data:/models
    restart: "no"

volumes:
  ollama_data:
    driver: local
  # ⬇️ 추가: GGUF/Modelfile 공유 저장용
  models_data:
    driver: local